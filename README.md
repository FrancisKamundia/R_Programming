# R_Programming
Effective advertising using R for Data Science

 ABSTRACT

  Making an investment decision in a new or existing business isn’t easy. You need to identify a profitable niche if you starting a business and discover the right demographic of people to target. One can use specific advertisements to target the right audience, an online growing industry with over 4.5 billion internet users in the world. Machine learning is a technique in which systems getting to act without any explicit programming. The dataset used was taken from respondents aged 25-45 years whose area income majorly was around 80,000. The aim of this research is to identify the right ads according to variables such as ad topic line, age, area income, city, clicked on ad, daily internet usage, daily time spent on site, male and timestamp, for specific individuals using machine learning techniques. Marketing & Advertising: Stats and Data Analysis. (2016, July 26)

INTRODUCTION

  Online advertising has been growing rapidly each year because of the dramatically development of digital channel and the wide range of internet users. It allows advertisers to bid and pay for measurable user responses, such as clicks on ads hence making click prediction systems central to most online advertising systems. It enables the advertiser to expand or rather increase their target market since nowadays people use more than one screen devices which may consist of mobiles, computers, or television in one time to perform their task. Machine learning plays an ultimate role in computing the expected utility of an individual ad to a user, and in this way increases the efficiency of the marketplace. This research study consists of 10 factors: 'Daily Time Spent on Site', 'Age', 'Area Income', 'Daily Internet Usage', 'Ad Topic Line', 'City', 'Male', 'Country', Timestamp' and 'Clicked on Ad'. The fundamental variable we are keen on is 'Clicked on Ad'. This variable can have two potential results: 0 and 1 where 0 implies to the situation where a client didn't tap the advertisement, while 1 implies to the situation where a client taps the advertisement. This research checks whether we can utilize the other 9 factors to precisely foresee the worth 'Clicked on Ad' factor as well as use the exploratory analysis to perceive how 'Daily Time Spent on Site' in combination with 'Ad Topic Line' influences the client's decision to tap on the add. Therefore, the goal of this research is to explore the effectiveness of advertisement analysis for specific individuals using machine learning techniques.
 

RESEARCH METHODOLOGY
DEFINING THE RESEARCH QUESTION

  The research project is about analyzing data to establish the category of people who click on advertisement links the most. This is by using data analysis to establish this. According to a McKinsey report as recently as two or three years ago, the key challenges for data-analytics leaders were getting their senior teams to understand its potential, finding enough talent to build models, and creating the right data fabric to tie together the often-disparate databases inside and outside the enterprise. But as these professionals have pushed for scale, new challenges have emerged. McKinsey & Company (March 2015).
  Data analysis and statistics traditionally play an important role in analyzing the success of companies and brands in the market. The growth of internet marketing has introduced new trends. Marketing and advertising remain hotspots of big data analytics. Stats and data analysis are used by companies to keep progress of their performance in the market and keep an eye on the ongoing trends, as well as indicators of growth and important changes in the market.
  According to a data science central report, Worldwide revenue in digital advertising in 2016 has reached 198,438.9 million USD. The largest segment of the Digital Advertising market is Search Advertising, with the volume of 90,740.3 million USD in 2016. Compared to 2015, when the revenue was 168,422 million USD, all segments have recorded steady growth, and the trend is expected to continue in 2017. After Search Advertising, largest segments remain Banner Advertising, with 43,467 million USD in 2016, and Social Media Advertising with 27, 065 million USD in 2016. Mobile generated revenue has grown from 56,940 million USD in 2015 to 81,069 USD in 2016, which is 7.1 percent. Display advertising stats by industry in 2016 show that 13.9 percent revenue is generated in retail industry, which amounts to 8,920 million USD. This is followed by Fast Moving Consumer Goods (FMCD) with 13.4 and Services with 13.44 percent. United States remain a country which generated the most revenue, 80,177 million USD, followed by China with 43,902, United Kingdom (14,251) and Japan (8,671). Maliachi, A. (2020, May 28).

UNDERSTANDING THE DATA

  The data used was is in csv form and has 1000 records. I went through the data to establish if the records have been recorded in order and well to make it easy for the codes to run well.
  To better understand data, it is crucial to make use of data visualization tools. The graphics package is used for plotting base graphs like scatter plot, box plot etc. A complete list of functions with help pages can be obtained by typing : library(help = "graphics"). The plot() function is a kind of a generic function for plotting of R objects.
  In a bar plot, data is represented in the form of rectangular bars and the length of the bar is proportional to the value of the variable or column in the dataset. Both horizontal, as well as a vertical bar chart, can be generated by tweaking the horiz parameter.
  A histogram is quite similar to a bar chart except that it groups values into continuous ranges. A histogram represents the frequencies of values of a variable bucketed into ranges.
  We have seen how the summary() command in R can display the descriptive statistics for every variable in the dataset. Boxplot does the same albeit graphically in the form of quartiles. It is again very straightforward to plot a boxplot in R. Pandey, P. (2020, September 11).
  There is a very interesting feature in R which enables us to plot multiple charts at once. This comes in very handy during the EDA since the need to plot multiple graphs one by one is eliminated. For drawing a grid, the first argument should specify certain attributes like the margin of the grid(mar), no of rows and columns (mfrow), whether a border is to be included (bty) and position of the labels (las: 1 for horizontal, las: 0 for vertical). Kaplan, D. T. (2015).
  The ggplot2 package is one of the most widely used visualization packages in R. It enables the users to create sophisticated visualizations with little code using the Grammar of Graphics. The Grammar of Graphics is a general scheme for data visualization which breaks up graphs into semantic components such as scales and layers. Pandey, P. (2020, September 11).
  Plotly is an R package that creates interactive web-based graphs via the open source JavaScript graphing library plotly.js. It can easily translate the ‘ggplot2’ graphs to web-based versions also.
  Geographic data (Geo data) relates to the location-based data. It primarily deals with describing objects with respect to their relationship in space. The data is usually stored in the form of coordinates. It makes more sense to be able to see a state or a country in the form of a map as it gives a more realistic overview.
In a nutshell, there are many ways to represent data in R with the use of the various data visualization tools. In this report, we shall explore mainly the ggplot function.

DATA CLEANING

  The analysis will be considered a success when interactive visualizations that tell a good story are achieved. An all-round Exploratory data analysis is achieved and a powerful predictive model will be considered a success. The role of the analysis is to come up with recommendations which the entrepreneur will use to executed in order to identify the fertile areas to first invest on.
  I used the library (reader) function to provide a fast and friendly way to read rectangular data from the csv file. The function is designed to flexibly parse many types of data found in the wild, while still cleanly failing when data unexpectedly changes. I then imported the data using the line df <- read_csv("C:/Users/advert.csv").
  After importation, I have to define the relevant columns which are daily time spent on site, age, area income, daily internet usage, ad topic line, city, male, country, timestamp and clicked on ad. After which, I imported the library tidyverse function. The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. See how the tidyverse makes data science faster, easier and more fun with “R for Data Science”. The relevant packages which I would make use of are, ggplot2 3.3.3, v dplyr   1.0.2, tibble  3.0.4, v stringr 1.4.0, v tidyr 1.1.2, v forcats 0.5.0, v purrr   0.3.4. These packages will help me make sense of the data by mapping out graphs and plots on my workbook.
  To kick off the data clean up, I made use of as_tibble() function which turns an existing object, such as a data frame or matrix, into a so-called tibble, a data frame with class tbl_df. The data clean up exercise led to 990 records of data from the 1,000 that were initially there. To rename column names where they have spaces in between, I inserted the below code.
  The sum(!complete.cases(my_data)) function removed null values from the data set while sum(duplicated(my_data)) ensured there are no missing values from the dataset. The result of this, after running the code was “## [1] 0” which means that there are no duplicate values from the data set.
1.	Exploratory Data Analysis
To explore the data set, I generated plots on my work to establish correlation of age, income bracket, sex and daily internet usage on number of clicks on the adds.
To establish correlation between age and clicks on ads, I used a bloxplot having run the boxplot function with the below parameters;
boxplot(eliminated$Daily_Internet_Usage, main="Daily_Internet_Usage", boxwex=0.1)
 
 The plot shows that there were no outliers from the age column. Same model being run on data with income of the respondents brought about the below boxplot
 There were several outliers detected on the Area _income column and removed.
On the daily internet usage, I found out that there were no outliers. After cleaning the dataset there are 991 rows and 10 columns left.
 Best way to explore the data further is by using univariate analysis where we will find the measures of central tendency and measure of dispersion on each column of focus.
We start with daily internet usage where I got a mean of 65.05689, median of 68.41, mode of 62.26, variance of 252.8258, 15.9005 as standard deviation, minimal value of 32.6, maximum value of 91.43, range as 32.60-91.43, and below quantile;
  To have the best plot of the measures of central tendency and dispersion, we use the smooth scatter function which is an auxiliary function which evaluates the loess smooth at evaluation equally spaced points covering the range of x. Pandey, P. (2020, September 11). The result was as below;
  The findings were the daily time spend on site is scattered it does not exhibit a unique character such as clustering at specific times. The subsequent analysis was on age where mean is 35.98587, medium is 35, mode is 31, variance is 77.52303, standard deviation is 8.804716, minimum value is 19, maximum value is 61, range is 19 61, and the quantile is as below;
  We use the histogram plot for ease of analysis going by the kind of data which is rather simple and in whole numbers by making use of the hist function and passing elimated$age as its parameters. The result is shown in the below plot
  From the plot, we find that the age or respondents was mostly between 25 to 45 years.
As for areas of income the mean 55349.1, median 57260.41, mode 61833.9, variance 168000385, standard deviation 12961.5, minimal value 19991.72, maximum value 79484.8, range 19991.72 -79484.80, and quantile as shown below;
  To further explore the income data, we use ggplot function of R. ggplot2 is a plotting package that makes it simple to create complex plots from data in a data frame. It provides a more programmatic interface for specifying what variables to plot, how they are displayed, and general visual properties. The plot is as below;
  The finding is that the area income for the respondents is majorly ranging between 80000
For the daily internet usage, the mean is 179.9846, median 183.43, mode 167.22, variance 1940.743, standard deviation 44.05386, minimum value 104.78, maximum value is 269.96, range is 104.78 - 269.96, and quantile as shown below. 
  To plot the data, we use ggplot and incorporate geom_vline function which allows for annotation of the plot with vertical lines. Pandey, P. (2020, September 11).
  We find that respondents daily internet usage is above 175 (the mean value). A look into the gender data gives us a mean of 0.4793138, medium of 0, mode of 0, variance of 0.2498242, standard deviation of 0.4998241, minimum value of 0, maximum value of 1, range of 0-1, and a quantile as illustrated below; 
  To better illustrate the findings, we use ggplots and geom_bar function. Geom_bar() makes the height of the bar proportional to the number of cases in each group (or if the weight aesthetic is supplied, the sum of the weights).
  From the graph, we find that most respondents were females, represented by 0 as shown in the above diagram. For the clicked on add data, the mean is 0.4954591, median is 0, mode is 0, variance is 0.2502319, standard deviation is 0.5002318, minimum value is 0, maximum value is 1, range is 0-1 and quantile is as per below image;
  The click on add data is quite unique and there is no better way to illustrate it in a plot other than using a pie chart. In so doing, one has to incorporate plotdatalabel, ggplot, geombar and theme functions to have a better color code on the plot as illustrated below;
  We find that the distribution of the participants clicking the ad is 50% to 50%. To analyze data on the countries, best way to go about it is to calculate the frequency of countries as registered in the data set. We first import the dplyr library. dplyr is a new package which provides a set of tools for efficiently manipulating datasets in R. dplyr is the next iteration of plyr, focusing on only data frames. The library groups data together. From there, we then run the df code which outputs the resultant data and the below output is what we get from doing this.
  From this point, it is only prudent to run an overall correlation matrix so as to establish any other correlations that may exist in the data set. The first correlation matrix we run is on daily time spend on the site and the age where the covariance is -46.59899 meaning that there is negative linear relationship between the two variables. An increase in one result to a decrease in the other variable and vice versa. STHDA. (2019, May 12. The correlation is -0.3328515 meaning that data on daily time spent on a site and age are negatively linearly related. Below plot shows the correlation between the two data;
  Running a correlation matrix between age and clicking the ad gives us a covariance of 2.172663 which indicates a positive linear relationship between the two variables. Thus, if one increases the other also increases. For example, if the age increases the probability id high that the respondent will click the ad. We also get a correlation of 0.4932938 which means that the variables are positively linearly related. Plotting this on a graph with the use of a ggplot function leveraging on the facet_wrap function gives us the below result. A facet_wrap() functions wraps a 1d sequence of panels into 2d. This is generally a better use of screen space than facet_grid() because most displays are roughly rectangular;
  A correlation analysis on gender and number of clicks gives as a covariance of -0.01044756 and a correlation of -0.04178558 which means that there is a negative relationship between the gender and the rate of clicking the ad and there is a weak negative relationship between the two. This gives us the below plot which reveals that females clicked more ads than their male counterparts;

MODELLING

  The goal of a model is to provide a simple low-dimensional summary of a dataset. In the context of this book we’re going to use models to partition data into patterns and residuals. Strong patterns will hide subtler trends, so we’ll use models to help peel back layers of structure as we explore a dataset.
  However, before we can start using models on interesting, real, datasets, you need to understand the basics of how models work. For that reason, this chapter of the book is unique because it uses only simulated datasets. These datasets are very simple, and not at all interesting, but they will help you understand the essence of modelling before you apply the same techniques to real data in the next chapter.
  There are two parts to a model:
1.	First, you define a family of models that express a precise, but generic, pattern that you want to capture. For example, the pattern might be a straight line, or a quadratic curve. You will express the model family as an equation like y = a_1 * x + a_2 or y = a_1 * x ^ a_2. Here, x and y are known variables from your data, and a_1 and a_2 are parameters that can vary to capture different patterns.
2.	Next, you generate a fitted model by finding the model from the family that is the closest to your data. This takes the generic model family and makes it specific, like y = 3 * x + 7 or y = 9 * x ^ 2.
  To build a simple model, we will look at the simulated dataset sim1, included with the modelr package. It contains two continuous variables, x and y. Let’s plot them to see how they’re related:
  You can see a strong pattern in the data. Let’s use a model to capture that pattern and make it explicit. It’s our job to supply the basic form of the model. In this case, the relationship looks linear, i.e. y = a_0 + a_1 * x. Let’s start by getting a feel for what models from that family look like by randomly generating a few and overlaying them on the data. For this simple case, we can use geom_abline() which takes a slope and intercept as parameters. Later on we’ll learn more general techniques that work with any model.
  Since the project is a classification problem, I will use decition trees and Naive bayes supervised learning algorithims.
For the decision trees, we will first break the data into training & test sets using the below code:
  We shall then fit the model. Finally, we will plot the model tree using the below lines of code. The resultant is a tree as shown in the code. From the tree, it is crucial to run a performance evaluation of the mode. We do this by first loading the lattice R package in our work and attaching the caret package. We then mask purr package and implement the confusion matrix as illustrated below. The resultant performance output is 95% which is considerably good.
  To further illustrate data science modelling, we shall make use of the Naïve Bayes Algorithm. Naïve Bayes is a probabilistic machine learning algorithm based on the Bayes Theorem, used in a wide variety of classification tasks. Despite the advances in Machine Learning in the last years, it has proven to not only be simple but also fast, accurate, and reliable. It has been successfully used for many purposes, but it works particularly well with natural language processing (NLP) problems.
  Naïve Bayes is a probabilistic machine learning algorithm based on the Bayes Theorem, used in a wide variety of classification tasks. In this article, we will understand the Naïve Bayes algorithm and all essential concepts so that there is no room for doubts in understanding. Bayes’ Theorem is a simple mathematical formula used for calculating conditional probabilities. Conditional probability is a measure of the probability of an event occurring given that another event has (by assumption, presumption, assertion, or evidence) occurred. The fundamental Naïve Bayes assumption is that each feature makes an independent and equal contribution to the outcome. For this case, we shall create objects x which holds the predictor variables and y which holds the response variables. We then load the e1071 package that holds the Naive Bayes function and then fit the Naive Bayes model. We shall then conduct a model evaluation of the program;
 
RECOMMENDATION

  The entrepreneur should invest in advertising on a target group. This is because from the analysis we have observed that the younger generation takes more time compare to the middle-aged individuals but their rate of clicking the app is minimal. Thus, she should target the middle-aged people. I have also observed that the most gender that clicked the ad are the females who happen to spend more time on the ad compared to males.

REFERENCES

1) Maliachi, A. (2020, May 28). Data science within the advertising industry, Towards Data Science | Towards Data Science. Medium. https://towardsdatascience.com/data-science-within-the-advertising-industry-469c4d728635
2) Marketing & Advertising: Stats and Data Analysis. (2016, July 26). Data Science Central. https://www.datasciencecentral.com/profiles/blogs/marketing-advertising-stats-and-data-analysis
3) McKinsey & Company (March 2015). Marketing & Sales Big Data, Analytics, and the Future of Marketing & Sales
4) Pandey, P. (2020, September 11). A Comprehensive Guide to Data Visualization in R for Beginners. Medium. https://towardsdatascience.com/a-guide-to-data-visualisation-in-r-for-beginners-ef6d41a34174
5) Kaplan, D. T. (2015). Data Computing: An Introduction to Wrangling and Visualization with R (Preview ed.). Project MOSAIC.
6) Correlation matrix : A quick start guide to analyze, format and visualize a correlation matrix using R software - Easy Guides - Wiki - STHDA. (2019, May 12). Sthda. http://www.sthda.com/english/wiki/correlation-matrix-a-quick-start-guide-to-analyze-format-and-visualize-a-correlation-matrix-using-r-software 
